{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdig110/PythonFirstAttempt/blob/main/Gradient_Descent_Equations_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9e0617",
      "metadata": {
        "id": "8e9e0617"
      },
      "source": [
        "# Gradient Descent Equations in LaTeX\n",
        "Here are the equations related to gradient descent written in LaTeX.\n",
        "\n",
        "## Mean Squared Error\n",
        "$$\n",
        "\\text{mse} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - (mx_i + b) \\right)^2\n",
        "$$\n",
        "\n",
        "## Partial Derivative with respect to \\(m\\)\n",
        "$$\n",
        "\\frac{\\partial}{\\partial m} = \\frac{2}{n} \\sum_{i=1}^{n} -x_i \\left( y_i - (mx_i + b) \\right)\n",
        "$$\n",
        "\n",
        "## Partial Derivative with respect to \\(b\\)\n",
        "$$\n",
        "\\frac{\\partial}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} - \\left( y_i - (mx_i + b) \\right)\n",
        "$$\n",
        "\n",
        "## Update Rule for \\(m\\)\n",
        "$$\n",
        "m = m - \\text{learning rate} \\times \\frac{\\partial}{\\partial m}\n",
        "$$\n",
        "\n",
        "## Update Rule for \\(b\\)\n",
        "$$\n",
        "b = b - \\text{learning rate} \\times \\frac{\\partial}{\\partial b}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learning rate parameter is an arbitrary number we should play with. We first adjust it to 0.001 and will check if the cost function start decreasing or not. If so, we will then add to the learning curve to speed up the convergence rate. a good practice is to first keep the iterations lee for example 10 rather than 1000 to make sure it decreases using the learning rate.\n",
        "\n",
        "with 0.01 is the cost function goes down, showing the green light that we can increase the learning rate to say 0.1. with 0.1, the cost function goes up meaning that we passed the minumum. so we have to reduce it. we try 0.09 but the function still increae. we drop it to 0.08 and we start observing the cost function going down. so now that we have found the optimum learning rate number, we increase the iterations.\n"
      ],
      "metadata": {
        "id": "6v31j9JkDf1y"
      },
      "id": "6v31j9JkDf1y"
    },
    {
      "cell_type": "code",
      "source": [
        "# The learning rate parameter is an arbitrary number that we need to experiment with.\n",
        "# Initially, we set it to 0.001 and check if the cost function starts decreasing.\n",
        "# If it does, we then increase the learning rate to speed up the convergence rate.\n",
        "# A good practice is to start with fewer iterations, such as 10 instead of 1000,\n",
        "# to ensure that the cost function decreases with the given learning rate.\n",
        "\n",
        "# If the cost function decreases with a learning rate of 0.01, it indicates that\n",
        "# we can increase the learning rate to 0.1. However, if the cost function increases\n",
        "# with a learning rate of 0.1, it means we have surpassed the minimum.\n",
        "# We then reduce the learning rate to 0.09, but if the cost function still increases,\n",
        "# we drop it further to 0.08. At this point, if the cost function starts decreasing,\n",
        "# we have found the optimal learning rate.\n",
        "\n",
        "# With the optimal learning rate identified, we can increase the number of iterations.\n"
      ],
      "metadata": {
        "id": "3GchZZ3GFZK0"
      },
      "id": "3GchZZ3GFZK0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(x,y):\n",
        "    m_curr = b_curr = 0\n",
        "    iterations = 10000\n",
        "    n = len(x)\n",
        "    learning_rate = 0.08\n",
        "\n",
        "    for i in range(iterations):\n",
        "        y_predicted = m_curr * x + b_curr\n",
        "        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n",
        "        md = -(2/n)*sum(x*(y-y_predicted))\n",
        "        bd = -(2/n)*sum(y-y_predicted)\n",
        "        m_curr = m_curr - learning_rate * md\n",
        "        b_curr = b_curr - learning_rate * bd\n",
        "        print (\"m {}, b {}, cost {} iteration {}\".format(m_curr,b_curr,cost, i))\n",
        "\n",
        "x = np.array([1,2,3,4,5])\n",
        "y = np.array([5,7,9,11,13])\n",
        "\n",
        "gradient_descent(x,y)\n"
      ],
      "metadata": {
        "id": "9-Q0LCYaDVxo"
      },
      "id": "9-Q0LCYaDVxo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}